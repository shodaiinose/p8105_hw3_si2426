---
title: "p8105_homework 3"
output: github_document
date: "2022-10-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(viridis)
library(patchwork)
```

## Problem 2

#### Loading and Cleaning Data
```{r, message = FALSE}
accel_df = read_csv("./data/accel_data.csv") %>% janitor::clean_names() %>% pivot_longer(-c(week, day_id, day), names_to = "minute", values_to = "activity_counts") %>% mutate(minute = as.integer(gsub('[activity_]', '', minute))) %>% mutate(weekday_v_weekend = ifelse(day == "Saturday" | day == "Sunday", "Weekend", "Weekday")) %>% mutate(day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")), weekday_v_weekend = as.factor(weekday_v_weekend)) %>% arrange(week, day) %>% group_by(week, day) %>% mutate(day_number = cur_group_id())
```

In order to clean the data, the variable names were cleaned using `janitor::clean_names()`. The function `pivot_longer()` was used to tidy the data and create a row for each observation (each minute was given an entry in the newly created `minute` variable). The corresponding activity count values were added to the `activity_counts` variable. The "activity_" prefix in the `minute` variable were removed to have a integer variable that only contained the numeric values of the minutes. A `weekday_v_weekend` variable was created to distinguish between the types of days. The `day` variable was coerced into a factor variable based on the days of the week, starting with Monday. The dataset was then arranged by `week` and `day` order, meaning the first day would be the Monday in week 1 which is followed by the Tuesday in week 1 (this was done as the original dataset did not have the days ordered correctly within each week). A new variable called `day_number` was created to list and number the days in sequential order. The resulting dataset contains `r nrow(accel_df)` observations (one minute for each of the 35 days). The dataset contains the following variables: `week` to represent the week number, `day_id` to represent the day number (ranging from 1 to 35), `day` variable to represent the day of the week, `minute` to represent which minute of the day, `activity_counts` to represent the activity at that specific minute, and `weekday_v_weekend` to represent the type of day. The mean activity count per minute was `r format(mean(accel_df$activity_counts), scientific = F, digits = 6)`. 

#### Total Activity Over the Days
We will now aggregate across minutes and create a table showing the total activity per day.
```{r, message = FALSE}
total_activity_df = accel_df %>%
  group_by(day_number, week, day) %>%
  summarize(total_activity = sum(activity_counts))

total_activity_df %>%
  knitr::kable(digits = 2)
```

It can be noted that, according to the table, the man's activity levels were the lowest on the fourth and fifth Saturdays. On these days, his activity count were valued at 1440. This could indicate some type of reporting error; however, this will need to be investigated further. These values were much lower than the mean activity level per day, which was approximately `r format(mean(total_activity_df$total_activity), scientific = F, digits = 6)`. In order to study the patterns of activity level each day more effectively, a scatterplot using `day_id` as the x-axis and `total_activity` as the y-axis will be created. `geom_smooth` will be used to study how the the activity level changes across the 5 weeks.

```{r, message = FALSE}
ggplot(total_activity_df, aes(x = day_number, y = total_activity)) + 
  geom_point(aes(color = day)) +
  geom_smooth(se = FALSE) +  labs(
    title = "Total Activity Plot",
    x = "Day Number",
    y = "Total Activity Per Day",
  )
```
Viewing the plot, it is apparent that the individual was gradually increasing his activity levels until the second week, after which his activity levels decreased. This continued until around the third week when it continued to remain stable. It should be noted that there are several outliers, such as the fourth and fifth Saturdays, which have much lower activity levels than the other days. 

#### Activity Over the Course of the Day
```{r, message = FALSE}
accel_hour_df = accel_df %>% 
  mutate(hour = floor(minute/60)) %>% 
  group_by(hour, day, day_number) %>% 
  summarize(hourly_total = sum(activity_counts)) %>% arrange(day_number)

accel_hour_plot = ggplot(accel_hour_df, aes(x = hour, y = hourly_total)) + 
  geom_point(aes(color = day)) +
  geom_smooth(se = FALSE) +  labs(
    title = "Activity Over the Course of the Day (Hours)",
    x = "Hour (military time)",
    y = "Activity")

accel_min_plot = ggplot(accel_df, aes(x = minute, y = activity_counts)) + 
  geom_point(aes(color = day)) +
  geom_smooth(se = FALSE) +  labs(
    title = "Activity Over the Course of the Day (Minutes)",
    x = "Minute",
    y = "Activity")

accel_min_plot + accel_hour_plot
```

# WRITE SUMMARY

## Problem 3

```{r}
library(p8105.datasets)
data("ny_noaa")
```
The data contains `r nrow(ny_noaa)` observations. The dataset contains variables such as `id`, `date`, `prcp` to represent the precipitation, `snow` to represent the snowfall, `snwd` to represent the snow depth, `tmax` to represent the maximum temperature, and `tmin` to represent the minimum temperature. The dataset contains observations from `r count(ny_noaa %>% distinct(id))` distinct stations. There are `r sum(is.na(ny_noaa))` missing values in the dataset. `r sum(is.na(ny_noaa$id))` of these are from the `id` variable and `r sum(is.na(ny_noaa$date))` of these are from the `date` variable. From the remaining 5 variables, there should be `r  format(nrow(ny_noaa) * 5, scientific = F, digits = 8)` cells with data if the dataset were fully filled. However, from the 5 key variables with measurements, `r round((sum(is.na(ny_noaa)))/((nrow(ny_noaa)) * 5), 4) * 100`% of the cells contain missing values. This could post a significant issue, as over a quarter of the data from the 5 key variables is missing measurements. However, due to the large size of the dataset, there are still many observations that can be used in the analysis.

```{r}
ny_noaa_df = ny_noaa %>% separate(col = date, into = c("year", "month", "day")) %>% mutate(year = as.integer(year), month = as.integer(month), day = as.integer(day)) %>% mutate(prcp = prcp/10, tmax = as.numeric(tmax)/10, tmin = as.numeric(tmin)/10)

snow_counts_df = ny_noaa_df %>% group_by(snow) %>% select(snow) %>% summarize(count = n()) %>% arrange(-count)
```
In order to clean the data, separate variables for `year`, `month`, an `day` were created. These variables were then coerced to integer types. The `prcp` variable was mutated to be measured in mm, in order to be consistent with the other measurement units of the other variables. The `tmax` and `tmin` variables were mutated to be measured in degrees C. The most frequent value of snowfall was `r snow_counts_df[1, 1]` mm. The second most frequent value was `r snow_counts_df[2, 1]` mm. This is likely due to the fact that many days without snow were recorded. 

#### Average Max Temperature in January and July in Each Station Across Years
```{r, message = FALSE}
ny_noaa_df_jan_jul = ny_noaa_df %>% filter(month == 1 | month == 7) %>% group_by(id, month, year) %>% drop_na(tmax) %>% summarize(mean_tmax = mean(tmax, na.rm = FALSE))

ggplot(ny_noaa_df_jan_jul, aes(x = year, y = (mean_tmax), color = id )) + 
  geom_point() +
  geom_line() + 
  facet_grid(. ~month) +   
  theme(legend.position = "none")
```
# WRITE SUMMARY



#### Tmax vs. Tmin and Snowfall Distribution Plot
```{r}
tmax_v_tmin = ggplot(ny_noaa_df, aes(x = tmin, y = tmax)) + 
  geom_hex()

noaa_density = ny_noaa_df %>% 
  filter(snow > 0 & snow < 100) %>% 
  group_by(year) %>%
  ggplot(aes(x = snow, fill = year, group = year)) + 
  geom_density(alpha = .1, adjust = .5) +
  scale_fill_viridis()

tmax_v_tmin + noaa_density
```
# WRITE SUMMARY








